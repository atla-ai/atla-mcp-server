"""MCP server implementation."""

import asyncio
import os
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import AsyncIterator, Dict, List, Literal, Optional

from atla import AsyncAtla
from mcp.server.fastmcp import FastMCP

# Create the MCP server
mcp = FastMCP("AtlaEvaluator")

# Initialize Atla client
# Note: API key will be taken from environment variable ATLA_API_KEY
atla_async_client = AsyncAtla(api_key=os.environ.get("ATLA_API_KEY"))


# config


@dataclass
class MCPState:
    """State of the MCP server."""

    atla_client: AsyncAtla


# models


# tools


async def evaluate_llm_response(
    model_input: str,
    model_output: str,
    evaluation_criteria: str,
    expected_model_output: Optional[str] = None,
    model_context: Optional[str] = None,
    model_id: Literal["atla-selene", "atla-selene-mini"] = "atla-selene",
) -> Dict[str, str]:
    """Evaluate an LLM response on a given criterion using Atla's evaluation model.

    This function returns a dictionary containing the evaluation score and critique.

    Args:
        model_input (str): The input or prompt given to the model to generate a response.
        model_output (str): The output or response generated by the model, which needs
            to be evaluated.
        evaluation_criteria (str): The specific criterion or instructions on which to
            evaluate the model output.
        expected_model_output (Optional[str]): A reference or ideal answer to compare
            against the model output. Defaults to None.
        model_context (Optional[str]): Additional context or information provided to the
            model during generation. Defaults to None.
        model_id (Literal["atla-selene", "atla-selene-mini"]): The Atla model ID to use
            for evaluation. Defaults to "atla-selene".

    Returns:
        Dict[str, str]: A dictionary containing two keys:
            - "score": A numerical evaluation score assigned to the model output.
            - "critique": A textual critique or feedback on the model output.
    """
    result = await atla_async_client.evaluation.create(
        model_id=model_id,
        model_input=model_input,
        model_output=model_output,
        evaluation_criteria=evaluation_criteria,
        expected_model_output=expected_model_output,
        few_shot_examples=[],
        model_context=model_context,
    )

    return {
        "score": result.result.evaluation.score,
        "critique": result.result.evaluation.critique,
    }


@mcp.tool()
async def evaluate_llm_response_on_multiple_criteria(
    model_input: str,
    model_output: str,
    evaluation_criteria_list: List[str],
    expected_model_output: Optional[str] = None,
    model_context: Optional[str] = None,
    model_id: Literal["atla-selene", "atla-selene-mini"] = "atla-selene",
) -> List[Dict[str, str]]:
    """Evaluate an LLM response on a given criteria using Atla's evaluation model.

    This function returns a dictionary containing the evaluation score and critique.

    Args:
        model_input (str): The input or prompt given to the model to generate a response.
        model_output (str): The output or response generated by the model, which needs
            to be evaluated.
        evaluation_criteria_list (List[str]): A list of criteria on which to evaluate
            the model output. Each item should be a string describing a specific
            criterion on which to evaluate the model's output.
        expected_model_output (Optional[str]): A reference or ideal answer to compare
            against the model output. Defaults to None.
        model_context (Optional[str]): Additional context or information provided to the
            model during generation. Defaults to None.
        model_id (Literal["atla-selene", "atla-selene-mini"]): The Atla model ID to use
            for evaluation. Defaults to "atla-selene".

    Returns:
        List[Dict[str, str]]: A list of dictionaries, each containing evaluation results:
            - "score": A numerical evaluation score assigned to the model output.
            - "critique": A textual critique or feedback on the model output.
    """
    tasks = [
        evaluate_llm_response(
            model_input=model_input,
            model_output=model_output,
            evaluation_criteria=criterion,
            expected_model_output=expected_model_output,
            model_context=model_context,
            model_id=model_id,
        )
        for criterion in evaluation_criteria_list
    ]
    results = await asyncio.gather(*tasks)
    return results


# app factory


def app_factory(atla_api_key: str) -> FastMCP:
    """Factory function to create an Atla MCP server with the given API key."""

    @asynccontextmanager
    async def lifespan(server: FastMCP) -> AsyncIterator[MCPState]:
        async with AsyncAtla(api_key=atla_api_key) as client:
            yield MCPState(atla_client=client)

    mcp = FastMCP("Atla", lifespan=lifespan)
    mcp.tool()(evaluate_llm_response)
    mcp.tool()(evaluate_llm_response_on_multiple_criteria)

    return mcp
